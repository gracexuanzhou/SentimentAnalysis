{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG4X6Rpg6Oq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42861a89-8686-4c56-e8a9-92866b733971"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# stemming package\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "# lemmatization package\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# stopwords package\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tree import Tree\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50JudltEWO1r",
        "outputId": "2f752581-5745-46b3-ce3b-bde0455002e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIRcUe7lWjWV"
      },
      "source": [
        "biden_sample_df = pd.read_csv(\"/content/gdrive/My Drive/webA2/archive/cleaned_Biden_171236.csv\",encoding = \"ISO-8859-1\", lineterminator='\\n')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FZCMLWlXufp",
        "outputId": "23901112-c50d-428a-c6e9-85c7e571357a"
      },
      "source": [
        "print(\"biden sample data set size: \", len(biden_sample_df))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "biden sample data set size:  171236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psPTXQ1_XzA2"
      },
      "source": [
        "import re\n",
        "# Reading contractions.csv and storing it as a dict.\n",
        "contractions = pd.read_csv('/content/gdrive/My Drive/webA2/archive/contractions.csv', index_col='Contraction')\n",
        "contractions.index = contractions.index.str.lower()\n",
        "contractions.Meaning = contractions.Meaning.str.lower()\n",
        "contractions_dict = contractions.to_dict()['Meaning']\n",
        "\n",
        "# Defining regex patterns.\n",
        "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
        "userPattern       = '@[^\\s]+'\n",
        "hashtagPattern    = '#[^\\s]+'\n",
        "alphaPattern      = \"[^A-Za-z0-9<>]\"\n",
        "sequencePattern   = r\"(.)\\1\\1+\"\n",
        "seqReplacePattern = r\"\\1\\1\"\n",
        "\n",
        "# Defining regex for emojis\n",
        "smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n",
        "sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n",
        "neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n",
        "lolemoji          = r\"[8:=;]['`\\-]?p+\"\n",
        "\n",
        "def preprocess_apply(tweet):\n",
        "\n",
        "    #tweet = tweet.lower()\n",
        "\n",
        "    # Replace all URls with '<url>'\n",
        "    tweet = re.sub(urlPattern,'<url>',tweet)\n",
        "    # Replace @USERNAME to '<user>'.\n",
        "    tweet = re.sub(userPattern,'<user>', tweet)\n",
        "    \n",
        "    # Replace #Hashtags to '<hashtags>'.\n",
        "    # note that i don't remove hashtag during training, so ~ \n",
        "    #tweet = re.sub(hashtagPattern,'<hashtag>', tweet)\n",
        "    \n",
        "    # Replace 3 or more consecutive letters by 2 letter.\n",
        "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "\n",
        "    # Replace all emojis.\n",
        "    tweet = re.sub(r'<3', '<heart>', tweet)\n",
        "    tweet = re.sub(smileemoji, '<smile>', tweet)\n",
        "    tweet = re.sub(sademoji, '<sadface>', tweet)\n",
        "    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n",
        "    tweet = re.sub(lolemoji, '<lolface>', tweet)\n",
        "\n",
        "    for contraction, replacement in contractions_dict.items():\n",
        "        tweet = tweet.replace(contraction, replacement)\n",
        "\n",
        "    # Remove non-alphanumeric and symbols\n",
        "    tweet = re.sub(alphaPattern, ' ', tweet)\n",
        "\n",
        "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
        "    tweet = re.sub(r'/', ' / ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UNPmyUWfjyT",
        "outputId": "0630a1e8-4ccf-4fde-c406-688f40b60eb7"
      },
      "source": [
        "# do preprocess, and store in a new column, in df\n",
        "%%time\n",
        "biden_sample_df['processed_text'] = biden_sample_df.tweet.apply(preprocess_apply)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14.4 s, sys: 32.9 ms, total: 14.4 s\n",
            "Wall time: 14.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gutVjsGUFQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqhnx_cHfrpQ",
        "outputId": "80e1b07a-5fce-4a75-ca3b-367192183de7"
      },
      "source": [
        "# have a look at processed text\n",
        "print(\"Raw text: \")\n",
        "print(biden_sample_df.tweet[15])\n",
        "print(\"Processed text:\")\n",
        "print(biden_sample_df.processed_text[15])\n",
        "print(\"Raw text: \")\n",
        "print(biden_sample_df.tweet[19])\n",
        "print(\"Processed text:\")\n",
        "print(biden_sample_df.processed_text[19])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw text: \n",
            "@TwitterSafety You donÃ¢ÂÂt usually bother, why the huge effort in response to the #Biden @nypost story?\n",
            "Processed text:\n",
            "<user> You don      t usually bother  why the huge effort in response to the  Biden <user> story \n",
            "Raw text: \n",
            "Has this awoken you from your slumbers yet @BBCJonSopel. Getting to the point where even you may have to file a negative story about #Biden! https://t.co/Y54y25n6dU\n",
            "Processed text:\n",
            "Has this awoken you from your slumbers yet <user> Getting to the point where even you may have to file a negative story about  Biden  <url>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ldO_qpWE-mS"
      },
      "source": [
        "def get_NER(postag_text, NER_List):\n",
        "    #get NER\n",
        "    # could try : https://github.com/flairNLP/flair\n",
        "    # https://blog.csdn.net/qq_27713281/article/details/72819219 nltk.ne_chunk(tags)#tags是句子词性标注后的结果，同样是句子级\n",
        "    chunked = nltk.ne_chunk(postag_text)\n",
        "    # nltk.ne_chunk returns a nested nltk.tree.Tree object so you would have to traverse the Tree object to get to the NEs\n",
        "    #http://www.itkeyword.com/doc/1722997285955701853/nltk-named-entity-recognition-to-a-python-list\n",
        "\n",
        "    for i in chunked:\n",
        "        if type(i) == Tree:\n",
        "\n",
        "            chunk_label = i.label()\n",
        "            chunk_string = \" \".join([token for token, pos in i.leaves()])\n",
        "            NER_List.append((chunk_string, chunk_label))\n",
        "            #print(current_chunk)\n",
        "        '''\n",
        "        if current_chunk:\n",
        "            named_entity = \" \".join(current_string)\n",
        "            if named_entity not in continuous_chunk:\n",
        "                continuous_chunk.append(named_entity)\n",
        "                NER_List.append()\n",
        "                current_chunk = []\n",
        "        else:\n",
        "            continue\n",
        "        '''\n",
        "\n",
        "    return\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "def NLProcess(text):\n",
        "\t# tokenize, remove punctuation, remove stopwords\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\t#intermediate = tokenizer.tokenize(text)\n",
        "    sent_text = sent_tokenize(text)\n",
        "   # print (sent_text)\n",
        "    #分词word tokenize：使用nltk.word_tokenize(text)\n",
        "\t#print(sent_text)\n",
        "    #nltk.sent_tokenize(text) #按句子分割\n",
        "    #nltk.word_tokenize(sentence) #分词\n",
        "    #nltk的分词是句子级别的，所以对于一篇文档首先要将文章按句子进行分割，然后句子进行分词\n",
        "    #http://www.pythontip.com/blog/post/10012/\n",
        "    NER_token = []\n",
        "    NER_remove_long_token =[]\n",
        "    for sent in sent_text:\n",
        "        word_token = tokenizer.tokenize(sent)\n",
        "     #   print (word_token)\n",
        "\t\t#word_token = tokenizer.tokenize(sent)\n",
        "        all_stopwords = stopwords.words('english')\n",
        "        intermediate = [w for w in word_token if not w in all_stopwords]\n",
        "\n",
        "\n",
        "\t# ==== Stemming process =====\n",
        "\t\t#porter = PorterStemmer()\n",
        "\t\t#lancaster = LancasterStemmer()\n",
        "\t\t#word_token = [lancaster.stem(i) for i in word_token]\n",
        "\t\t#intermediate = [porter.stem(i) for i in word_token]\n",
        "        #删除停用词\n",
        "\n",
        "\n",
        "#==== pos taging =======\n",
        "#先用NLTK包的pos_tag方法（part-of-speech tagging ）来对单词的词性进行标记，标记后的结果是二元数组格式。之后从这个二元数列中挑出我们所有需要的tag，存放进一个二元数列。\n",
        "\n",
        "        postag_token = nltk.pos_tag(intermediate)\n",
        "      #  print (postag_token)\n",
        "\t\t#print(postag_token)\n",
        "\n",
        "\n",
        "\n",
        "        get_NER(postag_token, NER_token)\n",
        "\t\t# remove the entity mentions which contains more than 3 words,\n",
        "\t\t#but not with capital alphabet\n",
        "\n",
        "\n",
        "        return NER_token\n",
        "\n",
        "# this function is for method 2 of parse_html\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9r56Au-jdqK",
        "outputId": "b51720af-30c2-4638-b1b9-4249a6373e9d"
      },
      "source": [
        "text = \" Has this awoken you from your slumbers yet <user> Getting to the point where even you may have to file a negative story about  Biden  <url>\"\n",
        "NLProcess(text)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' Has this awoken you from your slumbers yet <user> Getting to the point where even you may have to file a negative story about  Biden  <url>']\n",
            "['Has', 'this', 'awoken', 'you', 'from', 'your', 'slumbers', 'yet', 'user', 'Getting', 'to', 'the', 'point', 'where', 'even', 'you', 'may', 'have', 'to', 'file', 'a', 'negative', 'story', 'about', 'Biden', 'url']\n",
            "[('Has', 'NNP'), ('awoken', 'VBN'), ('slumbers', 'NNS'), ('yet', 'RB'), ('user', 'VBP'), ('Getting', 'VBG'), ('point', 'NN'), ('even', 'RB'), ('may', 'MD'), ('file', 'VB'), ('negative', 'JJ'), ('story', 'NN'), ('Biden', 'NNP'), ('url', 'NN')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Biden', 'PERSON')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpjNiR5fGWB4",
        "outputId": "17b221e6-8706-451a-8be1-d35bf18b1fac"
      },
      "source": [
        "%%time\n",
        "biden_sample_df[\"ner\"] = biden_sample_df.processed_text.apply(NLProcess)\n",
        "biden_sample_df.to_csv(\"/content/gdrive/My Drive/webA2/archive/biden_sample_ner3.csv\", index=False)\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20min 28s, sys: 15.2 s, total: 20min 43s\n",
            "Wall time: 20min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zDVfF1aHJy_"
      },
      "source": [
        "# load model \n",
        "model = load_model(\"/content/gdrive/My Drive/webA2/archive/BiLSTM_gensim_0839_15epo_100wdataset.h5\")"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwxSRwONHf5I"
      },
      "source": [
        "# load the tokenizer\n",
        "import pickle\n",
        "# loading tokenizer\n",
        "with open('/content/gdrive/My Drive/webA2/archive/Tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ywfq99jHwl9"
      },
      "source": [
        "# predict function\n",
        "def predict(text):\n",
        "    \n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=60)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "    # Decode sentiment\n",
        "    #label = -1 if score < 0.5 else 1\n",
        "    out_score = round(float(score),4)\n",
        "\n",
        "    return out_score"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaD7g_ZtI9ZV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvfGQ3uOI4Ni"
      },
      "source": [
        "biden_sample_df1 = pd.read_csv(\"/content/gdrive/My Drive/webA2/archive/biden_sample_ner3.csv\",encoding = \"ISO-8859-1\", lineterminator='\\n')"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrD2_gB3H2PK",
        "outputId": "cce78262-b333-4df0-f0e2-0568dd9dcd06"
      },
      "source": [
        "%%time\n",
        "biden_sample_df1[\"predict_score_bi\"] = biden_sample_df1.processed_text.apply(lambda x: predict(x))\n",
        "# store the result\n",
        "biden_sample_df1.to_csv(\"/content/gdrive/My Drive/webA2/archive/biden_scored.csv\", index=False)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2h 48min 36s, sys: 2min 48s, total: 2h 51min 24s\n",
            "Wall time: 2h 27min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InC-pdWMuPDN"
      },
      "source": [
        "import pandas as pd\n",
        "biden_sored = pd.read_csv(\"/content/gdrive/My Drive/webA2/archive/biden_scored.csv\",encoding = \"ISO-8859-1\", lineterminator='\\n')\n",
        "alphaPattern      = \"[^A-Za-z0-9<>]\"\n",
        "org               = \"ORGANIZATION\"\n",
        "per               = \"PERSON\"\n",
        "gpe               = \"GPE\"\n",
        "\n",
        "def NER_process (ner):\n",
        "  ner = re.sub(alphaPattern, '', ner)\n",
        "  ner = re.sub(org, ',', ner)\n",
        "  ner = re.sub(per, ',', ner)\n",
        "  ner = re.sub(gpe, ',', ner)\n",
        "\n",
        "  return ner\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NFpNdq9_5ngI",
        "outputId": "2916f0ac-b741-4407-f0b5-709264fe55ad"
      },
      "source": [
        "ner = \"[('PLEASE', 'ORGANIZATION'), ('RIGHT', 'ORGANIZATION'), ('Move', 'PERSON'), ('Orange Monkey', 'ORGANIZATION'), ('Biden PLEASE', 'PERSON')]\"\n",
        "NER_process(ner)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'PLEASE,RIGHT,Move,OrangeMonkey,BidenPLEASE,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1i0fLAW6gMe",
        "outputId": "22b05f52-9b37-4c36-c523-884afaca76e4"
      },
      "source": [
        "%%time\n",
        "biden_sored[\"entity\"] = biden_sored.ner.apply(NER_process)\n",
        "biden_sored.to_csv(\"/content/gdrive/My Drive/webA2/archive/biden_sample_ner4.csv\", index=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.36 s, sys: 145 ms, total: 5.5 s\n",
            "Wall time: 5.93 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR-idNFo-3yX",
        "outputId": "b1ad1b61-f5f0-4aee-87cc-534f4ff6daf0"
      },
      "source": [
        "print(type(biden_sored.ner[0]))\n",
        "print(biden_sored.head)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "<bound method NDFrame.head of                  created_at  ...                                             entity\n",
            "0       2020-10-15 00:00:01  ...  Elecciones2020EnFlorida,DonaldTrump,PembrokePi...\n",
            "1       2020-10-15 00:00:20  ...                           Biden,TrumpIsNotAmerica,\n",
            "2       2020-10-15 00:00:22  ...                                  HunterBidenBiden,\n",
            "3       2020-10-15 00:00:25  ...  NYPost,CENSORED,USGSPJoeBidenTrump,China,Twitter,\n",
            "4       2020-10-15 00:00:57  ...  FBI,HunterBidenComputerDataUkraineDealingsRepo...\n",
            "...                     ...  ...                                                ...\n",
            "171231  2020-11-03 23:59:45  ...  JoeBiden,Vote2020BidenHarris2020Amen,SaveOurCo...\n",
            "171232  2020-11-03 23:59:49  ...                                     ElectionNight,\n",
            "171233  2020-11-03 23:59:50  ...                      Biden,Elections2020Biden2020,\n",
            "171234  2020-11-03 23:59:54  ...        DNC,JoeBiden,BidenFamilyBobulinski,Rosneft,\n",
            "171235  2020-11-03 23:59:57  ...     YOU,BidenVoteBlueToEndTheNightmareTrump,THATS,\n",
            "\n",
            "[171236 rows x 17 columns]>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}