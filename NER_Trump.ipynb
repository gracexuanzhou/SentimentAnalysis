{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NER_Trump.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1VpXJ3_-eMYXXuohBB2G_LdQoQCuU4ZnW","authorship_tag":"ABX9TyMUQz4n+7STLl/OrLKDN0CF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"fcYduwFqHHQH"},"source":["import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import state_union\n","from nltk.tokenize import PunktSentenceTokenizer\n","from nltk.tokenize import RegexpTokenizer\n","# stemming package\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","# lemmatization package\n","from nltk.stem import WordNetLemmatizer\n","# stopwords package\n","from nltk.corpus import stopwords\n","from nltk.tree import Tree\n","\n","from tensorflow.keras.models import load_model\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import LabelEncoder\n","\n","import pandas as pd\n","import re\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ievKaCkVb6Mh"},"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('state_union')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffHex4MnPk0c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607274562328,"user_tz":-60,"elapsed":549,"user":{"displayName":"Evelyn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZjqziWZA8lLwmJQyRU-ZZMecgSv-AqXwThtU=s64","userId":"02300020295037684536"}},"outputId":"84f8457b-4184-4493-f166-229d8c8e0611"},"source":["#read file \n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cPj6OJDHRLei"},"source":["trump_sample_df = pd.read_csv(\"/content/gdrive/MyDrive/WDPS/cleaned_Trump_255441.csv\",encoding = \"ISO-8859-1\", lineterminator='\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoQw0B7WR5SG","executionInfo":{"status":"ok","timestamp":1607274576488,"user_tz":-60,"elapsed":604,"user":{"displayName":"Evelyn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZjqziWZA8lLwmJQyRU-ZZMecgSv-AqXwThtU=s64","userId":"02300020295037684536"}},"outputId":"88294df8-692c-4c76-f354-fe83ce1b80c4"},"source":["print(\"trump sample data set size: \", len(trump_sample_df))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["trump sample data set size:  255441\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cfCCJN2Sdzqs"},"source":["\n","# Reading contractions.csv and storing it as a dict.\n","contractions = pd.read_csv('/content/gdrive/MyDrive/WDPS/contractions.csv', index_col='Contraction')\n","contractions.index = contractions.index.str.lower()\n","contractions.Meaning = contractions.Meaning.str.lower()\n","contractions_dict = contractions.to_dict()['Meaning']\n","\n","# Defining regex patterns.\n","urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n","userPattern       = '@[^\\s]+'\n","hashtagPattern    = '#[^\\s]+'\n","alphaPattern      = \"[^A-Za-z0-9<>]\"\n","sequencePattern   = r\"(.)\\1\\1+\"\n","seqReplacePattern = r\"\\1\\1\"\n","\n","# Defining regex for emojis\n","smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n","sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n","neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n","lolemoji          = r\"[8:=;]['`\\-]?p+\"\n","\n","def preprocess_apply(tweet):\n","\n","    #tweet = tweet.lower()\n","\n","    # Replace all URls with '<url>'\n","    tweet = re.sub(urlPattern,'<url>',tweet)\n","    # Replace @USERNAME to '<user>'.\n","    tweet = re.sub(userPattern,'<user>', tweet)\n","    \n","    # Replace #Hashtags to '<hashtags>'.\n","    # note that i don't remove hashtag during training, so ~ \n","    #tweet = re.sub(hashtagPattern,'<hashtag>', tweet)\n","    \n","    # Replace 3 or more consecutive letters by 2 letter.\n","    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n","\n","    # Replace all emojis.\n","    tweet = re.sub(r'<3', '<heart>', tweet)\n","    tweet = re.sub(smileemoji, '<smile>', tweet)\n","    tweet = re.sub(sademoji, '<sadface>', tweet)\n","    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n","    tweet = re.sub(lolemoji, '<lolface>', tweet)\n","\n","    for contraction, replacement in contractions_dict.items():\n","        tweet = tweet.replace(contraction, replacement)\n","\n","    # Remove non-alphanumeric and symbols\n","    tweet = re.sub(alphaPattern, ' ', tweet)\n","\n","    # Adding space on either side of '/' to seperate words (After replacing URLS).\n","    tweet = re.sub(r'/', ' / ', tweet)\n","    return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtRJbU9CgM4c","executionInfo":{"status":"ok","timestamp":1607274606502,"user_tz":-60,"elapsed":21722,"user":{"displayName":"Evelyn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZjqziWZA8lLwmJQyRU-ZZMecgSv-AqXwThtU=s64","userId":"02300020295037684536"}},"outputId":"10288525-a81f-4883-844f-eba3c7e22bfe"},"source":["# do preprocess, and store in a new column, in df\n","%%time\n","trump_sample_df['processed_text'] = trump_sample_df.tweet.apply(preprocess_apply)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 21.1 s, sys: 60 ms, total: 21.2 s\n","Wall time: 21.2 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dU_AYPZWgWB6"},"source":["# have a look at processed text\n","print(\"Raw text: \")\n","print(trump_sample_df.tweet[798])\n","print(\"Processed text:\")\n","print(trump_sample_df.processed_text[798])\n","print(\"Raw text: \")\n","print(trump_sample_df.tweet[2567])\n","print(\"Processed text:\")\n","print(trump_sample_df.processed_text[2567])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"goJSz_7bVQAh"},"source":["def get_NER(postag_text, NER_List):\n","    # get NER\n","    # could try : https://github.com/flairNLP/flair\n","    chunked = nltk.ne_chunk(postag_text)\n","\n","    for i in chunked:\n","        if type(i) == Tree:\n","            chunk_label = i.label()\n","            chunk_string = \" \".join([token for token, pos in i.leaves()])\n","            NER_List.append((chunk_string, chunk_label))\n","\n","    return\n","\n","\n","def NLProcess(text):\n","    # tokenize, remove punctuation, remove stopwords\n","     tokenizer = RegexpTokenizer(r'\\w+')\n","     #intermediate = tokenizer.tokenize(text)\n","     sent_text = sent_tokenize(text)\n","     #print(sent_text)\n","     NER_token = []\n","     NER_remove_long_token = []\n","     for sent in sent_text:\n","        word_token = tokenizer.tokenize(sent)\n","        #print (word_token)\n","        # word_token = tokenizer.tokenize(sent)\n","        all_stopwords = stopwords.words('english')\n","        intermediate = [w for w in word_token if not w in all_stopwords]\n","        \n","        for sent in sent_text:\n","          word_token = word_tokenize(sent)\n","        # word_token = tokenizer.tokenize(sent)\n","\n","        # ==== Stemming process =====\n","        # porter = PorterStemmer()\n","        # lancaster = LancasterStemmer()\n","        # word_token = [lancaster.stem(i) for i in word_token]\n","        # intermediate = [porter.stem(i) for i in word_token]\n","        \n","        intermediate = [w for w in word_token if not w in stopwords.words('english')]\n","        # ==== pos taging =======\n","        postag_token = nltk.pos_tag(intermediate)\n","        #print (postag_token)\n","\n","        get_NER(postag_token, NER_token)\n","        \n","        for mention in NER_token:\n","          if len(mention[0].split()) < 3 or mention[0].isupper():\n","            NER_remove_long_token.append(mention)\n","\n","        return NER_remove_long_token\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlpRs7mCDYni","executionInfo":{"status":"ok","timestamp":1607278461157,"user_tz":-60,"elapsed":2532787,"user":{"displayName":"Evelyn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZjqziWZA8lLwmJQyRU-ZZMecgSv-AqXwThtU=s64","userId":"02300020295037684536"}},"outputId":"a1d66dbb-94ae-4f65-8234-9e9f63e0330a"},"source":["%%time\n","trump_sample_df[\"ner\"] =trump_sample_df.processed_text.apply(NLProcess)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 40min 45s, sys: 1min 24s, total: 42min 10s\n","Wall time: 42min 12s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4vcOX6MK-4n6"},"source":["trump_sample_df.to_csv(\"/content/gdrive/MyDrive/WDPS/trump_sample_ner1.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUHwgmCUiwrv","executionInfo":{"status":"ok","timestamp":1607282054188,"user_tz":-60,"elapsed":561,"user":{"displayName":"Evelyn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZjqziWZA8lLwmJQyRU-ZZMecgSv-AqXwThtU=s64","userId":"02300020295037684536"}},"outputId":"1198cbf7-7922-433f-86c3-e7074b4c1cf4"},"source":["print(trump_sample_df.ner[78934])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('Trump', 'PERSON')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2P_HotqpIZoL"},"source":["# load model \n","model = load_model(\"/content/gdrive/MyDrive/WDPS/BiLSTM_gensim_0839_15epo_100wdataset.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cg-ASF9HIoMk"},"source":["# load the tokenizer\n","import pickle\n","# loading tokenizer\n","with open('/content/gdrive/MyDrive/WDPS/Tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRXz65xqIw7Z"},"source":["# predict function\n","def predict(text):\n","    \n","    # Tokenize text\n","    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=60)\n","    # Predict\n","    score = model.predict([x_test])[0]\n","    # Decode sentiment\n","    #label = -1 if score < 0.5 else 1\n","    out_score = round(float(score),4)\n","\n","    return out_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KrX1wIiLK_8B"},"source":["trump_sample_df1 = pd.read_csv(\"/content/gdrive/MyDrive/WDPS/trump_sample_ner1.csv\",encoding = \"ISO-8859-1\", lineterminator='\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TzYK_rjTI2qw","executionInfo":{"status":"ok","timestamp":1607222656855,"user_tz":-60,"elapsed":662,"user":{"displayName":"Evelyn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZjqziWZA8lLwmJQyRU-ZZMecgSv-AqXwThtU=s64","userId":"02300020295037684536"}},"outputId":"40d49698-67d0-4bb3-d219-ac42bce56067"},"source":["# try predict the sample dataset and estimate the time, cpu runtime type\n","%%time\n","trump_sample_df1[\"predict_score_bi\"] = trump_sample_df1.processed_text.apply(lambda x: predict(x))\n","# store the result\n","trump_sample_df1.to_csv(\"/content/gdrive/MyDrive/WDPS/trump_protext_scored1.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 4h 5min 25s, sys: 3min 49s, total: 4h 9min 15s\n","Wall time: 3h 29min 53s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D7OORnkONvL4"},"source":["\n","trump_sample_df3 = pd.read_csv(\"/content/gdrive/MyDrive/WDPS/trump_protext_scored1.csv\",encoding = \"ISO-8859-1\", lineterminator='\\n')\n","alphaPattern      = \"[^A-Za-z0-9<>]\"\n","org               = \"ORGANIZATION\"\n","per               = \"PERSON\"\n","gpe               = \"GPE\"\n","\n","def NER_process (ner):\n","  try:\n","    ner = re.sub(alphaPattern, '', ner)\n","    ner = re.sub(org, ',', ner)\n","    ner = re.sub(per, ',', ner)\n","    ner = re.sub(gpe, ',', ner)\n","    return ner\n","  except:\n","    print (\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"lGVth59aU_iJ","executionInfo":{"status":"ok","timestamp":1607282154311,"user_tz":-60,"elapsed":529,"user":{"displayName":"Evelyn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZjqziWZA8lLwmJQyRU-ZZMecgSv-AqXwThtU=s64","userId":"02300020295037684536"}},"outputId":"c184901e-cacb-4e10-fcf9-45e6af5e977e"},"source":["ner= \"[('NYPost', 'ORGANIZATION'), ('CENSORED', 'ORGANIZATION'), ('US', 'GSP'), ('JoeBiden Trump', 'ORGANIZATION'), ('China', 'GPE'), ('Twitter', 'PERSON')]\"\n","NER_process (ner)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'NYPost,CENSORED,USGSPJoeBidenTrump,China,Twitter,'"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BWvLte3ZVC0j","executionInfo":{"status":"ok","timestamp":1607282191807,"user_tz":-60,"elapsed":2415,"user":{"displayName":"Evelyn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZjqziWZA8lLwmJQyRU-ZZMecgSv-AqXwThtU=s64","userId":"02300020295037684536"}},"outputId":"b2b41b4b-3e14-49d4-b825-baecec12ed4e"},"source":["%%time\n","trump_sample_df3[\"entity\"] = trump_sample_df3.ner.apply(NER_process)\n","trump_sample_df3.to_csv(\"/content/gdrive/MyDrive/WDPS/trump_entity.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","CPU times: user 1.87 s, sys: 18.1 ms, total: 1.89 s\n","Wall time: 1.9 s\n"],"name":"stdout"}]}]}